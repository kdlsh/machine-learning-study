{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n",
      "(5, 4)\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "T, H = 5, 4\n",
    "hs = np.random.randn(T, H)\n",
    "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])\n",
    "\n",
    "ar = a.reshape(5, 1).repeat(4, axis=1)\n",
    "print(ar.shape)\n",
    "\n",
    "t = hs * ar\n",
    "print(t.shape)\n",
    "\n",
    "c = np.sum(t, axis=0)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n",
      "(10, 4)\n"
     ]
    }
   ],
   "source": [
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "a = np.random.randn(N, T)\n",
    "ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "\n",
    "t = hs * ar\n",
    "print(t.shape)\n",
    "\n",
    "c = np.sum(t, axis=1)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.np import *  # import numpy as np\n",
    "from common.layers import Softmax\n",
    "\n",
    "\n",
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ar = a.reshape(N, T, 1)#.repeat(T, axis=1)\n",
    "        t = hs * ar\n",
    "        c = np.sum(t, axis=1)\n",
    "\n",
    "        self.cache = (hs, ar)\n",
    "        return c\n",
    "\n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar\n",
    "        da = np.sum(dar, axis=2)\n",
    "\n",
    "        return dhs, da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        hr = h.reshape(N, 1, H)#.repeat(T, axis=1)\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)\n",
    "        a = self.softmax.forward(s)\n",
    "\n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "\n",
    "    def backward(self, da):\n",
    "        hs, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ds = self.softmax.backward(da)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        dhs = dt * hr\n",
    "        dhr = dt * hs\n",
    "        dh = np.sum(dhr, axis=1)\n",
    "\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "\n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:,t,:] = dh\n",
    "\n",
    "        return dhs_enc, dhs_dec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.time_layers import *\n",
    "from ch07.seq2seq import Encoder, Seq2seq\n",
    "from ch08.attention_layer import TimeAttention\n",
    "\n",
    "\n",
    "class AttentionEncoder(Encoder):\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout\n",
    "    \n",
    "class AttentionDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.attention = TimeAttention()\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, enc_hs):\n",
    "        h = enc_hs[:,-1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        dec_hs = self.lstm.forward(out)\n",
    "        c = self.attention.forward(enc_hs, dec_hs)\n",
    "        out = np.concatenate((c, dec_hs), axis=2)\n",
    "        score = self.affine.forward(out)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        N, T, H2 = dout.shape\n",
    "        H = H2 // 2\n",
    "\n",
    "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n",
    "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
    "        ddec_hs = ddec_hs0 + ddec_hs1\n",
    "        dout = self.lstm.backward(ddec_hs)\n",
    "        dh = self.lstm.dh\n",
    "        denc_hs[:, -1] += dh\n",
    "        self.embed.backward(dout)\n",
    "\n",
    "        return denc_hs\n",
    "\n",
    "    def generate(self, enc_hs, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        h = enc_hs[:, -1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([sample_id]).reshape((1, 1))\n",
    "\n",
    "            out = self.embed.forward(x)\n",
    "            dec_hs = self.lstm.forward(out)\n",
    "            c = self.attention.forward(enc_hs, dec_hs)\n",
    "            out = np.concatenate((c, dec_hs), axis=2)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(sample_id)\n",
    "\n",
    "        return sampled\n",
    "\n",
    "\n",
    "class AttentionSeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        args = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = AttentionEncoder(*args)\n",
    "        self.decoder = AttentionDecoder(*args)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 4.08\n",
      "| 에폭 1 |  반복 21 / 351 | 시간 10[s] | 손실 3.09\n",
      "| 에폭 1 |  반복 41 / 351 | 시간 20[s] | 손실 1.90\n",
      "| 에폭 1 |  반복 61 / 351 | 시간 31[s] | 손실 1.72\n",
      "| 에폭 1 |  반복 81 / 351 | 시간 41[s] | 손실 1.46\n",
      "| 에폭 1 |  반복 101 / 351 | 시간 51[s] | 손실 1.19\n",
      "| 에폭 1 |  반복 121 / 351 | 시간 62[s] | 손실 1.14\n",
      "| 에폭 1 |  반복 141 / 351 | 시간 72[s] | 손실 1.09\n",
      "| 에폭 1 |  반복 161 / 351 | 시간 83[s] | 손실 1.06\n",
      "| 에폭 1 |  반복 181 / 351 | 시간 93[s] | 손실 1.04\n",
      "| 에폭 1 |  반복 201 / 351 | 시간 103[s] | 손실 1.03\n",
      "| 에폭 1 |  반복 221 / 351 | 시간 114[s] | 손실 1.02\n",
      "| 에폭 1 |  반복 241 / 351 | 시간 124[s] | 손실 1.02\n",
      "| 에폭 1 |  반복 261 / 351 | 시간 134[s] | 손실 1.01\n",
      "| 에폭 1 |  반복 281 / 351 | 시간 145[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 301 / 351 | 시간 155[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 321 / 351 | 시간 165[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 341 / 351 | 시간 176[s] | 손실 1.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1978-08-11\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1978-08-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1978-08-11\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1978-08-11\n",
      "---\n",
      "정확도 0.000%\n",
      "| 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 1.00\n",
      "| 에폭 2 |  반복 21 / 351 | 시간 10[s] | 손실 1.00\n",
      "| 에폭 2 |  반복 41 / 351 | 시간 21[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 61 / 351 | 시간 31[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 81 / 351 | 시간 41[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 101 / 351 | 시간 52[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 121 / 351 | 시간 62[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 141 / 351 | 시간 72[s] | 손실 0.98\n",
      "| 에폭 2 |  반복 161 / 351 | 시간 82[s] | 손실 0.98\n",
      "| 에폭 2 |  반복 181 / 351 | 시간 93[s] | 손실 0.97\n",
      "| 에폭 2 |  반복 201 / 351 | 시간 103[s] | 손실 0.95\n",
      "| 에폭 2 |  반복 221 / 351 | 시간 113[s] | 손실 0.94\n",
      "| 에폭 2 |  반복 241 / 351 | 시간 124[s] | 손실 0.90\n",
      "| 에폭 2 |  반복 261 / 351 | 시간 134[s] | 손실 0.83\n",
      "| 에폭 2 |  반복 281 / 351 | 시간 144[s] | 손실 0.74\n",
      "| 에폭 2 |  반복 301 / 351 | 시간 154[s] | 손실 0.66\n",
      "| 에폭 2 |  반복 321 / 351 | 시간 165[s] | 손실 0.58\n",
      "| 에폭 2 |  반복 341 / 351 | 시간 175[s] | 손실 0.46\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 2006-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 2007-08-09\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1983-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 2016-11-08\n",
      "---\n",
      "정확도 51.640%\n",
      "| 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 3 |  반복 21 / 351 | 시간 10[s] | 손실 0.30\n",
      "| 에폭 3 |  반복 41 / 351 | 시간 21[s] | 손실 0.21\n",
      "| 에폭 3 |  반복 61 / 351 | 시간 31[s] | 손실 0.14\n",
      "| 에폭 3 |  반복 81 / 351 | 시간 41[s] | 손실 0.09\n",
      "| 에폭 3 |  반복 101 / 351 | 시간 52[s] | 손실 0.07\n",
      "| 에폭 3 |  반복 121 / 351 | 시간 62[s] | 손실 0.05\n",
      "| 에폭 3 |  반복 141 / 351 | 시간 73[s] | 손실 0.04\n",
      "| 에폭 3 |  반복 161 / 351 | 시간 83[s] | 손실 0.03\n",
      "| 에폭 3 |  반복 181 / 351 | 시간 93[s] | 손실 0.03\n",
      "| 에폭 3 |  반복 201 / 351 | 시간 104[s] | 손실 0.02\n",
      "| 에폭 3 |  반복 221 / 351 | 시간 114[s] | 손실 0.02\n",
      "| 에폭 3 |  반복 241 / 351 | 시간 124[s] | 손실 0.02\n",
      "| 에폭 3 |  반복 261 / 351 | 시간 135[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 281 / 351 | 시간 145[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 301 / 351 | 시간 156[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 321 / 351 | 시간 166[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 341 / 351 | 시간 176[s] | 손실 0.01\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 99.900%\n",
      "| 에폭 4 |  반복 1 / 351 | 시간 0[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 21 / 351 | 시간 10[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 41 / 351 | 시간 21[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 61 / 351 | 시간 31[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 81 / 351 | 시간 42[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 101 / 351 | 시간 52[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 121 / 351 | 시간 62[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 141 / 351 | 시간 73[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 161 / 351 | 시간 83[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 181 / 351 | 시간 93[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 201 / 351 | 시간 104[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 221 / 351 | 시간 114[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 241 / 351 | 시간 124[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 261 / 351 | 시간 135[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 281 / 351 | 시간 145[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 301 / 351 | 시간 156[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 321 / 351 | 시간 166[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 341 / 351 | 시간 176[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 99.900%\n",
      "| 에폭 5 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 21 / 351 | 시간 10[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 41 / 351 | 시간 21[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 61 / 351 | 시간 31[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 81 / 351 | 시간 42[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 101 / 351 | 시간 52[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 121 / 351 | 시간 62[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 141 / 351 | 시간 73[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 161 / 351 | 시간 83[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 181 / 351 | 시간 94[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 201 / 351 | 시간 104[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 221 / 351 | 시간 114[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 241 / 351 | 시간 125[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 261 / 351 | 시간 135[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 281 / 351 | 시간 145[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 301 / 351 | 시간 156[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 321 / 351 | 시간 166[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 341 / 351 | 시간 177[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 99.920%\n",
      "| 에폭 6 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 21 / 351 | 시간 10[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 41 / 351 | 시간 21[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 61 / 351 | 시간 31[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 81 / 351 | 시간 42[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 101 / 351 | 시간 52[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 121 / 351 | 시간 62[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 141 / 351 | 시간 73[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 161 / 351 | 시간 83[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 181 / 351 | 시간 93[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 201 / 351 | 시간 104[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 221 / 351 | 시간 114[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 241 / 351 | 시간 125[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 261 / 351 | 시간 135[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 281 / 351 | 시간 145[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 301 / 351 | 시간 156[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 321 / 351 | 시간 166[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 341 / 351 | 시간 176[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 99.920%\n",
      "| 에폭 7 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 21 / 351 | 시간 10[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 41 / 351 | 시간 21[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 61 / 351 | 시간 31[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 81 / 351 | 시간 42[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 101 / 351 | 시간 52[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 121 / 351 | 시간 62[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 141 / 351 | 시간 73[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 161 / 351 | 시간 83[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 181 / 351 | 시간 93[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 201 / 351 | 시간 104[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 221 / 351 | 시간 114[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 241 / 351 | 시간 124[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 261 / 351 | 시간 135[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 281 / 351 | 시간 145[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 301 / 351 | 시간 156[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 321 / 351 | 시간 166[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 341 / 351 | 시간 176[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 100.000%\n",
      "| 에폭 8 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 21 / 351 | 시간 10[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 41 / 351 | 시간 21[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 61 / 351 | 시간 31[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 81 / 351 | 시간 42[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 101 / 351 | 시간 52[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 121 / 351 | 시간 62[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 141 / 351 | 시간 73[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 161 / 351 | 시간 83[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 181 / 351 | 시간 93[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 201 / 351 | 시간 104[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 221 / 351 | 시간 114[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 241 / 351 | 시간 124[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 261 / 351 | 시간 135[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 281 / 351 | 시간 145[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 301 / 351 | 시간 156[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 321 / 351 | 시간 166[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 341 / 351 | 시간 176[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 100.000%\n",
      "| 에폭 9 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 21 / 351 | 시간 10[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 41 / 351 | 시간 21[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 61 / 351 | 시간 31[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 81 / 351 | 시간 42[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 101 / 351 | 시간 52[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 121 / 351 | 시간 62[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 141 / 351 | 시간 73[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 161 / 351 | 시간 83[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 181 / 351 | 시간 93[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 201 / 351 | 시간 104[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 221 / 351 | 시간 114[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 241 / 351 | 시간 124[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 261 / 351 | 시간 135[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 281 / 351 | 시간 145[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 301 / 351 | 시간 156[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 321 / 351 | 시간 166[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 341 / 351 | 시간 177[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 100.000%\n",
      "| 에폭 10 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 21 / 351 | 시간 10[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 41 / 351 | 시간 21[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 61 / 351 | 시간 31[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 81 / 351 | 시간 41[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 101 / 351 | 시간 52[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 121 / 351 | 시간 62[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 141 / 351 | 시간 73[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 161 / 351 | 시간 83[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 181 / 351 | 시간 93[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 201 / 351 | 시간 104[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 221 / 351 | 시간 114[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 241 / 351 | 시간 124[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 261 / 351 | 시간 135[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 281 / 351 | 시간 145[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 301 / 351 | 시간 156[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 321 / 351 | 시간 166[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 341 / 351 | 시간 176[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 100.000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 50640 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "c:\\program files\\python37\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 54253 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "c:\\program files\\python37\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 50640 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "c:\\program files\\python37\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 54253 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "c:\\program files\\python37\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 51221 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "c:\\program files\\python37\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 54869 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "c:\\program files\\python37\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 46020 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "c:\\program files\\python37\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 51221 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "c:\\program files\\python37\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 54869 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "c:\\program files\\python37\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 46020 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('ch07')\n",
    "sys.path.append('ch08')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "from attention_seq2seq import AttentionSeq2seq\n",
    "from ch07.seq2seq import Seq2seq\n",
    "from ch07.peeky_seq2seq import PeekySeq2seq\n",
    "\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 입력 문장 반전\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "max_grad = 5.0\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1,\n",
    "                batch_size=batch_size, max_grad=max_grad)\n",
    "\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct,\n",
    "                                    id_to_char, verbose, is_reverse=True)\n",
    "\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('정확도 %.3f%%' % (acc * 100))\n",
    "\n",
    "\n",
    "model.save_params()\n",
    "\n",
    "# 그래프 그리기\n",
    "x = np.arange(len(acc_list))\n",
    "plt.plot(x, acc_list, marker='o')\n",
    "plt.xlabel('에폭')\n",
    "plt.ylabel('정확도')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('ch08')\n",
    "import numpy as np\n",
    "from dataset import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "from attention_seq2seq import AttentionSeq2seq\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 입력 문장 반전\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "model.load_params()\n",
    "\n",
    "_idx = 0\n",
    "def visualize(attention_map, row_labels, column_labels):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.pcolor(attention_map, cmap=plt.cm.Greys_r, vmin=0.0, vmax=1.0)\n",
    "\n",
    "    ax.patch.set_facecolor('black')\n",
    "    ax.set_yticks(np.arange(attention_map.shape[0])+0.5, minor=False)\n",
    "    ax.set_xticks(np.arange(attention_map.shape[1])+0.5, minor=False)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xticklabels(row_labels, minor=False)\n",
    "    ax.set_yticklabels(column_labels, minor=False)\n",
    "\n",
    "    global _idx\n",
    "    _idx += 1\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "np.random.seed(1984)\n",
    "for _ in range(5):\n",
    "    idx = [np.random.randint(0, len(x_test))]\n",
    "    x = x_test[idx]\n",
    "    t = t_test[idx]\n",
    "\n",
    "    model.forward(x, t)\n",
    "    d = model.decoder.attention.attention_weights\n",
    "    d = np.array(d)\n",
    "    attention_map = d.reshape(d.shape[0], d.shape[2])\n",
    "\n",
    "    # 출력하기 위해 반전\n",
    "    attention_map = attention_map[:,::-1]\n",
    "    x = x[:,::-1]\n",
    "\n",
    "    row_labels = [id_to_char[i] for i in x[0]]\n",
    "    column_labels = [id_to_char[i] for i in t[0]]\n",
    "    column_labels = column_labels[1:]\n",
    "\n",
    "    visualize(attention_map, row_labels, column_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
