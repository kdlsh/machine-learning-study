{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 기울기\n",
    "\n",
    "- 기울기란? 모든 변수의 편미분을 벡터로 정리한 것.\n",
    "- 책내용 : 한 개 이상의 변수가 들어오는 경우를 대비해서 편미분 시켜주는 함수 소개\n",
    "- 다음페이지는 파이썬 코드...!\n",
    "\n",
    "$$f(x_0, x_1) = x_0^2 + x_1^2          \\left(\\frac{\\partial f}{\\partial x_0}, \\frac{\\partial f}{\\partial x_1}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 기울기 구하는 함수 선언\n",
    "import numpy as np\n",
    "\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2        \n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4                        # 해석적 미분을 하기위한 수치\n",
    "    grad = np.zeros_like(x)         # 빈 array 생성\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        fxh1 = f(x)\n",
    "        v = x[idx]\n",
    "        x[idx] = v + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = v - h\n",
    "        fxh2 = f(x)\n",
    "    \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = v\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8.]\n",
      "[0. 4.]\n",
      "[6. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 기울기 결과 예시\n",
    "import numpy as np\n",
    "grad1 = numerical_gradient(function_2, np.array([3.0, 4.0]))\n",
    "grad2 = numerical_gradient(function_2, np.array([0.0, 2.0]))\n",
    "grad3 = numerical_gradient(function_2, np.array([3.0, 0.0]))\n",
    "\n",
    "print(grad1)\n",
    "print(grad2)\n",
    "print(grad3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- 그림 4-9 실습 (gradient_2d.py)\n",
    "- 각 장소에서 기울기가 가리키는 방향은, 함수의 출력값을 가장 크게 줄이는 방향이다. \n",
    "- ->손실함수를 최소화 시키는 방향을 알수 있게 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 경사법 (경사 하강법)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- 기울기를 이용해서 최적의 매개변수 (가중치와 편향) 를 찾아내는 방법 중 하나\n",
    "- 최적의 매개변수란? \n",
    "\n",
    "- ->손실함수가 최솟값이 될때의 매개변수 값을 말함 (아래는 경사법의 수식)\n",
    "\n",
    "$$ x_0 = x_0 - \\eta{\\frac{\\partial f}{\\partial x_0}}$$\n",
    "$$ x_1 = x_1 - \\eta{\\frac{\\partial f}{\\partial x_1}}$$\n",
    "$$\\eta(에타) : 학습률$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 경사 하강법 함수\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    # f : 최적화 하려는 함수\n",
    "    # init_x : 초깃값\n",
    "    # lr : 학습률\n",
    "    # step_num : 반복횟수\n",
    "    \n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### WARNING_ (단점)\n",
    "- 함수가 극솟값, 최솟값, 또 안장점(saddle point) 이 되는 장소에서는 기울기가 0 이다\n",
    "  - 극솟값 : 극소적인 최솟값 (한정된 범위 내에서의 최솟값)\n",
    "  - 안장점 : 어느방향에서는 극댓값, 어느방향에서는 극솟값\n",
    "- 경사법의 단점은, 복잡한 함수에서 고원(plateau) 이라고하는 곳에 빠져서 학습이 제대로 진행이 안될 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NOTE_ (참고)\n",
    "- 경사법은 최솟값을 찾느냐 최댓값을 찾느냐에따라 이름이 달라진다\n",
    "- 최솟값을 찾는 경사법 : 경사 하강법\n",
    "- 최댓값을 찾는 경사법 : 경사 상승법\n",
    "- 결론 : 본질적으로 중요하지 않다고함. 보통 경사 하강법으로 등장한다고합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.34235971e+12, -3.96091057e+12])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습률이 너무 큰 예 : lr=10.0\n",
    "gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100)\n",
    "\n",
    "# 학습률이 너무 작은 예: lr=1e-10\n",
    "gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "- 그림 4-10 gradient_method.py 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NOTE_ (참고)\n",
    "- 학습률과 같은 매개변수를 하이퍼파라미터라고 한다 (사람이 직접 설정해야하는 것임)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 신경망에서의 기울기\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\mathbf{W} =  \\begin{pmatrix}\n",
    "w_{11} & w_{21} & w_{31} \\\\\n",
    "w_{12} & w_{22} & w_{32}\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\mathbf{\\frac{\\partial L}{\\partial W}} =  \\begin{pmatrix}\n",
    "\\frac{\\partial L}{\\partial w_{11}} & \\frac{\\partial L}{\\partial W_{21}} & \\frac{\\partial L}{\\partial W_{31}} \\\\\n",
    "\\frac{\\partial L}{\\partial w_{12}} & \\frac{\\partial L}{\\partial W_{22}} & \\frac{\\partial L}{\\partial W_{32}}\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  \n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 랜덤으로 w 생성\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.15875713 -2.43448087 -1.01921226]\n",
      " [ 2.15711139 -0.00924973  1.11667505]]\n"
     ]
    }
   ],
   "source": [
    "# 객체 선언\n",
    "net = simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.63665453 -1.46901328  0.39348018]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 x varidables 선언 및 내적 진행\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 내적의 최댓값 인덱스\n",
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3588208113426927"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정답 레이벌 t 선언 후 (원 핫 인코딩), 손실함수 계산하기\n",
    "t = np.array([0,0,1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.53447292  0.00880758 -0.54328051]\n",
      " [ 0.80170939  0.01321138 -0.81492076]]\n"
     ]
    }
   ],
   "source": [
    "# 기울기 값 계산 \n",
    "def f(W):\n",
    "        return net.loss(x,t)\n",
    "\n",
    "dW= numerical_gradient(f,net.W)\n",
    "print(dW)\n",
    "\n",
    "# 만약에, 경사 하강법을 적용한다면, \n",
    "# -> (직전의 가중치) - (기울기값*학습률) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 학습 알고리즘 구현하기 \n",
    "\n",
    "#### 0. 전제 : 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라고함\n",
    "#### 1. 미니배치 : 훈련 데이터 중 일부를 무작위로 가져옴. 이것을 미니배치라고 한다.\n",
    "#### 2. 기울기 산출 : 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매배변수의 기울기를 구한다.\n",
    "#### 3. 매개변수 갱신 : 가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\n",
    "#### 4. 반복 : 1~3 단계를 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2층 신경망 구현하기\n",
    "\n",
    "두 개의 레이어이므로 그 구성은 다음 그림과 같다. \n",
    "\n",
    "<img src=http://neuralnetworksanddeeplearning.com/images/tikz35.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 활성화 함수\n",
    "활성화 함수는 보통 실수 전체를 정의역으로 시그모이드 함수, 램프 함수, 맥스 아웃 함수 등이 있다.\n",
    "- 시그모이드 함수: 실수 전체를 정의역으로, (0, 1)을 치역으로 가진다.\n",
    "$$ f(u) = \\frac{1}{1 + e^{-u}} $$\n",
    "\n",
    "- 쌍곡선 정접 함수: (-1, 1)의 치역을 갖는다.\n",
    "$$ f(u) = tanh(u) $$\n",
    "\n",
    "- 램프 함수 (ramp function, rectified linear function): u < 0인 부분을 0으로 바꾼 단순 함수이다. 단순하고 계산량이 적다. 학습이 빠르고 최종결과도 더 좋은 경우가 많아 가장 많이 사용되고 있다.\n",
    "$$ f(u) = max(u, 0) $$\n",
    "\n",
    "- 맥스아웃 함수: 각각의 총 입력을 유닛별로 따로 계산한 후, 그 중한다. 최대값을 유닛의 출력으로 한다.   \n",
    "$$ f(u_j) = max (u_jk) (k=1,...,K) $$\n",
    "\n",
    "- 항등 사상: 회귀 문제를 위한 신경망에서 사용한다.\n",
    "$$ f(u) = u $$\n",
    "\n",
    "#### ★**소프트맥스 함수: 클래스 분류를 위한 신경망에서 사용한다. 출력의 합이 항상 1이 된다. 모든 유닛의 총 입력으로부터 결정되는 점이 다른 활성화 함수와 다르다. 지수 함수에 따른 오버플로우를 방지하기 위해, 보통 입력값 중 최대값을 기준으로 정규화한다. **\n",
    "$$ f(u) = \\frac{\\exp({u_k})}{\\sum_{j=1}^{k} \\exp({u_j})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  \n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "    # 초기설정\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01): \n",
    " \n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    # 활성화 함수 적용 및 소프트맥스(항등함수) 적용\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # 손실함수 계산 부분\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    # 정확도 계산 부분\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # 기울기 계산 부분\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    # 입력데이터, 정답레이블 설정\n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "iters_num = 1000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 1000번 돌린다. \n",
    "for i in range(iters_num):\n",
    "    # 600000개의 훈련데이터에서 100개를 무작위로 선택한다. \n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask] # 무작위로 선택된 100개의 입력 데이터\n",
    "    t_batch = t_train[batch_mask] # 그 결과값인 100개의 레이블 데이터\n",
    "    \n",
    "    # 미분에 의한 기울기 벡터를 구함: 속도가 매우 느리다.\n",
    "    # grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 오차 역전파에 의해 기울기 벡터를 구함\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        # 각각의 가중치를 초기 랜덤값에서 음의 기울기 방향으로 가중치를 갱신한다. (즉, 하강한다.)\n",
    "        # 이 때 학습률(learning rate)의 값으로 갱신량을 결정한다. 오버 피팅이 발생하지 않도록 적절한 값을 산정한다.\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # 학습 경과 \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 시험 데이터로 평가하기\n",
    "### NOTE_ \n",
    "- 에폭(epoch) 은 하나의 단위이다.\n",
    "- 훈련 데이터가 총 10,000개이고 100개를 하나의 미니배치로 학습할경우\n",
    "- 1 에폭은 100회가 된다. 그러니깐, 모든 훈련 데이터를 '소진' 하는게 1 에폭이다. \n",
    "- 다음페이지는 실제 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc: 0.102, test_acc: 0.101\n",
      "train_acc: 0.792, test_acc: 0.796\n",
      "train_acc: 0.878, test_acc: 0.882\n",
      "train_acc: 0.898, test_acc: 0.902\n",
      "train_acc: 0.910, test_acc: 0.912\n",
      "train_acc: 0.916, test_acc: 0.918\n",
      "train_acc: 0.921, test_acc: 0.922\n",
      "train_acc: 0.926, test_acc: 0.926\n",
      "train_acc: 0.929, test_acc: 0.930\n",
      "train_acc: 0.932, test_acc: 0.934\n",
      "train_acc: 0.935, test_acc: 0.937\n",
      "train_acc: 0.938, test_acc: 0.938\n",
      "train_acc: 0.941, test_acc: 0.939\n",
      "train_acc: 0.943, test_acc: 0.942\n",
      "train_acc: 0.945, test_acc: 0.945\n",
      "train_acc: 0.946, test_acc: 0.946\n",
      "train_acc: 0.948, test_acc: 0.948\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size/batch_size, 1)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 60000개의 훈련 데이터에서 임의로 100개를 선택한다.\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # 학습 경과 \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train_acc: {:0.3f}, test_acc: {:0.3f}\".format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 정리\n",
    "- 기계학습 사용 데이터는 훈련 데이터와 시험 데이터로 나눈다.\n",
    "- 훈련 데이터로 학습한 모델의 범용 능력을 시험 데이터로 평가한다.\n",
    "- 신경망 학습은 손실 함수를 지표로, 손실 함수 값이 작아지는 방향으로 매개변수를 갱신한다.\n",
    "- 가중치 매개변수를 갱신할 때는 기울기를 이용하고, 기울어진 방향으로 가중치 값을 갱신하는 작업을 반복한다.\n",
    "- 미분을 해서 가중치 매개변수의 기울기를 계산할 수 있다.\n",
    "- 다음 장에는 오차역전파법으로 고속으로 기울기를 계산하는 법을 배운다."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
