{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 기울기\n",
    "\n",
    "- 기울기란? 모든 변수의 편미분을 벡터로 정리한 것.\n",
    "- 책내용 : 한 개 이상의 변수가 들어오는 경우를 대비해서 편미분 시켜주는 함수 소개\n",
    "- 다음페이지는 파이썬 코드...!\n",
    "\n",
    "$$f(x_0, x_1) = x_0^2 + x_1^2          \\left(\\frac{\\partial f}{\\partial x_0}, \\frac{\\partial f}{\\partial x_1}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 기울기 구하는 함수 선언\n",
    "import numpy as np\n",
    "\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2        \n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4                        # 해석적 미분을 하기위한 수치\n",
    "    grad = np.zeros_like(x)         # 빈 array 생성\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        fxh1 = f(x)\n",
    "        v = x[idx]\n",
    "        x[idx] = v + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = v - h\n",
    "        fxh2 = f(x)\n",
    "    \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = v\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8.]\n",
      "[0. 4.]\n",
      "[6. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 기울기 결과 예시\n",
    "import numpy as np\n",
    "grad1 = numerical_gradient(function_2, np.array([3.0, 4.0]))\n",
    "grad2 = numerical_gradient(function_2, np.array([0.0, 2.0]))\n",
    "grad3 = numerical_gradient(function_2, np.array([3.0, 0.0]))\n",
    "\n",
    "print(grad1)\n",
    "print(grad2)\n",
    "print(grad3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- 그림 4-9 실습 (gradient_2d.py)\n",
    "- 각 장소에서 기울기가 가리키는 방향은, 함수의 출력값을 가장 크게 줄이는 방향이다. \n",
    "- ->손실함수를 최소화 시키는 방향을 알수 있게 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 경사법 (경사 하강법)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- 기울기를 이용해서 최적의 매개변수 (가중치와 편향) 를 찾아내는 방법 중 하나\n",
    "- 최적의 매개변수란? \n",
    "\n",
    "- ->손실함수가 최솟값이 될때의 매개변수 값을 말함 (아래는 경사법의 수식)\n",
    "\n",
    "$$ x_0 = x_0 - \\eta{\\frac{\\partial f}{\\partial x_0}}$$\n",
    "$$ x_1 = x_1 - \\eta{\\frac{\\partial f}{\\partial x_1}}$$\n",
    "$$\\eta(에타) : 학습률$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 경사 하강법 함수\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    # f : 최적화 하려는 함수\n",
    "    # init_x : 초깃값\n",
    "    # lr : 학습률\n",
    "    # step_num : 반복횟수\n",
    "    \n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### WARNING_ (단점)\n",
    "- 함수가 극솟값, 최솟값, 또 안장점(saddle point) 이 되는 장소에서는 기울기가 0 이다\n",
    "  - 극솟값 : 극소적인 최솟값 (한정된 범위 내에서의 최솟값)\n",
    "  - 안장점 : 어느방향에서는 극댓값, 어느방향에서는 극솟값\n",
    "- 경사법의 단점은, 복잡한 함수에서 고원(plateau) 이라고하는 곳에 빠져서 학습이 제대로 진행이 안될 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NOTE_ (참고)\n",
    "- 경사법은 최솟값을 찾느냐 최댓값을 찾느냐에따라 이름이 달라진다\n",
    "- 최솟값을 찾는 경사법 : 경사 하강법\n",
    "- 최댓값을 찾는 경사법 : 경사 상승법\n",
    "- 결론 : 본질적으로 중요하지 않다고함. 보통 경사 하강법으로 등장한다고합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.34235971e+12, -3.96091057e+12])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습률이 너무 큰 예 : lr=10.0\n",
    "gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100)\n",
    "\n",
    "# 학습률이 너무 작은 예: lr=1e-10\n",
    "gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "- 그림 4-10 gradient_method.py 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NOTE_ (참고)\n",
    "- 학습률과 같은 매개변수를 하이퍼파라미터라고 한다 (사람이 직접 설정해야하는 것임)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.4.2 신경망에서의 기울기\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\mathbf{W} =  \\begin{pmatrix}\n",
    "w_{11} & w_{21} & w_{31} \\\\\n",
    "w_{12} & w_{22} & w_{32}\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\mathbf{\\frac{\\partial L}{\\partial W}} =  \\begin{pmatrix}\n",
    "\\frac{\\partial L}{\\partial w_{11}} & \\frac{\\partial L}{\\partial W_{21}} & \\frac{\\partial L}{\\partial W_{31}} \\\\\n",
    "\\frac{\\partial L}{\\partial w_{12}} & \\frac{\\partial L}{\\partial W_{22}} & \\frac{\\partial L}{\\partial W_{32}}\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  \n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 랜덤으로 w 생성\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0473092   0.27156731 -0.34171092]\n",
      " [-0.24679116 -0.77143416 -0.04453794]]\n"
     ]
    }
   ],
   "source": [
    "# 객체 선언\n",
    "net = simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.25049757 -0.53135036 -0.24511069]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 x varidables 선언 및 내적 진행\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 내적의 최댓값 인덱스\n",
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0100394904277743"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정답 레이벌 t 선언 후 (원 핫 인코딩), 손실함수 계산하기\n",
    "t = np.array([0,0,1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.21734865  0.16412855 -0.3814772 ]\n",
      " [ 0.32602297  0.24619282 -0.5722158 ]]\n"
     ]
    }
   ],
   "source": [
    "# 기울기 값 계산 \n",
    "def f(W):\n",
    "        return net.loss(x,t)\n",
    "\n",
    "dW= numerical_gradient(f,net.W)\n",
    "print(dW)\n",
    "\n",
    "# 만약에, 경사 하강법을 적용한다면, \n",
    "# -> (직전의 가중치) - (기울기값*학습률) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.5 학습 알고리즘 구현하기 \n",
    "\n",
    "#### 0. 전제 : 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라고함\n",
    "#### 1. 미니배치 : 훈련 데이터 중 일부를 무작위로 가져옴. 이것을 미니배치라고 한다. \n",
    "#### (확률적 경사 하강법 stochastic gradient descent;SGD)\n",
    "#### 2. 기울기 산출 : 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매배변수의 기울기를 구한다.\n",
    "#### 3. 매개변수 갱신 : 가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\n",
    "#### 4. 반복 : 1~3 단계를 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.5.1 2층 신경망 구현하기\n",
    "\n",
    "두 개의 레이어이므로 그 구성은 다음 그림과 같다. \n",
    "\n",
    "<img src=http://neuralnetworksanddeeplearning.com/images/tikz35.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 활성화 함수\n",
    "활성화 함수는 보통 실수 전체를 정의역으로 시그모이드 함수, 램프 함수, 맥스 아웃 함수 등이 있다.\n",
    "- 시그모이드 함수: 실수 전체를 정의역으로, (0, 1)을 치역으로 가진다.\n",
    "$$ f(u) = \\frac{1}{1 + e^{-u}} $$\n",
    "\n",
    "- 쌍곡선 정접 함수: (-1, 1)의 치역을 갖는다.\n",
    "$$ f(u) = tanh(u) $$\n",
    "\n",
    "- 램프 함수 (ramp function, rectified linear function): u < 0인 부분을 0으로 바꾼 단순 함수이다. 단순하고 계산량이 적다. 학습이 빠르고 최종결과도 더 좋은 경우가 많아 가장 많이 사용되고 있다.\n",
    "$$ f(u) = max(u, 0) $$\n",
    "\n",
    "- 맥스아웃 함수: 각각의 총 입력을 유닛별로 따로 계산한 후, 그 중한다. 최대값을 유닛의 출력으로 한다.   \n",
    "$$ f(u_j) = max (u_jk) (k=1,...,K) $$\n",
    "\n",
    "- 항등 사상: 회귀 문제를 위한 신경망에서 사용한다.\n",
    "$$ f(u) = u $$\n",
    "\n",
    "#### ★**소프트맥스 함수: 클래스 분류를 위한 신경망에서 사용한다. 출력의 합이 항상 1이 된다. 모든 유닛의 총 입력으로부터 결정되는 점이 다른 활성화 함수와 다르다. 지수 함수에 따른 오버플로우를 방지하기 위해, 보통 입력값 중 최대값을 기준으로 정규화한다. **\n",
    "$$ f(u) = \\frac{\\exp({u_k})}{\\sum_{j=1}^{k} \\exp({u_j})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  \n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "    # 초기설정\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01): \n",
    " \n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    # 활성화 함수 적용 및 소프트맥스(항등함수) 적용\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # 손실함수 계산 부분\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    # 정확도 계산 부분\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # 기울기 계산 부분\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    # 입력데이터, 정답레이블 설정\n",
    "    # numerical_gradient 의 성능 개선판 다음장에서 구현\n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2 미니배치 학습 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.2910891812401983,\n",
       " 2.2989314458495027,\n",
       " 2.292371009931212,\n",
       " 2.2947126221225544,\n",
       " 2.298412562278942,\n",
       " 2.3009634749826136,\n",
       " 2.294486177497482,\n",
       " 2.2900055408862685,\n",
       " 2.2944853935048757,\n",
       " 2.295736305419078,\n",
       " 2.295814508336739,\n",
       " 2.286319506548349,\n",
       " 2.300806051193491,\n",
       " 2.289555801740117,\n",
       " 2.2844572841674284,\n",
       " 2.2866921645402742,\n",
       " 2.2775147938208935,\n",
       " 2.294517621166197,\n",
       " 2.2746885845804807,\n",
       " 2.2894823409272367,\n",
       " 2.300525462707183,\n",
       " 2.290952006517395,\n",
       " 2.2740603166199036,\n",
       " 2.287489362457792,\n",
       " 2.280958538589094,\n",
       " 2.291289259393349,\n",
       " 2.2846762381388355,\n",
       " 2.2940333138707585,\n",
       " 2.2877617416472,\n",
       " 2.299678928005436,\n",
       " 2.2852777892608267,\n",
       " 2.2854490807387076,\n",
       " 2.306859095460523,\n",
       " 2.301288600458965,\n",
       " 2.2855090665315583,\n",
       " 2.2867569214849457,\n",
       " 2.2946989974179655,\n",
       " 2.287457748002723,\n",
       " 2.292115524117121,\n",
       " 2.280897599531576,\n",
       " 2.278014683065989,\n",
       " 2.294652761346732,\n",
       " 2.2813802487157058,\n",
       " 2.300615834086,\n",
       " 2.2997918458420887,\n",
       " 2.295922373234268,\n",
       " 2.2884096294996414,\n",
       " 2.2891941335226815,\n",
       " 2.286022456575727,\n",
       " 2.2903132938231723,\n",
       " 2.289484116222762,\n",
       " 2.291353652209746,\n",
       " 2.2775632668551227,\n",
       " 2.283297780393114,\n",
       " 2.298808526231879,\n",
       " 2.2825645466141697,\n",
       " 2.303518099239447,\n",
       " 2.2937809425828632,\n",
       " 2.2683085329975814,\n",
       " 2.2941799828200926,\n",
       " 2.289321345244577,\n",
       " 2.2852324311210417,\n",
       " 2.285622939955332,\n",
       " 2.2877636903736516,\n",
       " 2.286562649461793,\n",
       " 2.2909563381891136,\n",
       " 2.2883288100606705,\n",
       " 2.2923861240726193,\n",
       " 2.2901980451624206,\n",
       " 2.2818942886155806,\n",
       " 2.291484551008141,\n",
       " 2.2718147841464122,\n",
       " 2.2874065146479445,\n",
       " 2.272044445508437,\n",
       " 2.2921579126035536,\n",
       " 2.2985344850410123,\n",
       " 2.2843563658565302,\n",
       " 2.274486695078222,\n",
       " 2.290198275951226,\n",
       " 2.2785125799389125,\n",
       " 2.293248258133139,\n",
       " 2.2761936134536453,\n",
       " 2.285511887131872,\n",
       " 2.29062037334242,\n",
       " 2.2838621834117347,\n",
       " 2.277606856306426,\n",
       " 2.2890550710585913,\n",
       " 2.284742338004884,\n",
       " 2.262880124590649,\n",
       " 2.2842536510674365,\n",
       " 2.2757965309301222,\n",
       " 2.281109390960178,\n",
       " 2.2807210356584573,\n",
       " 2.280908750503419,\n",
       " 2.272060771094465,\n",
       " 2.2756123775975174,\n",
       " 2.2904252407895505,\n",
       " 2.2848420944865193,\n",
       " 2.2887785354628725,\n",
       " 2.26159948889664,\n",
       " 2.2797019921300485,\n",
       " 2.272087718188401,\n",
       " 2.2803171897463157,\n",
       " 2.271675125478067,\n",
       " 2.2754857585151917,\n",
       " 2.264229610232757,\n",
       " 2.2678093223098172,\n",
       " 2.275054005381636,\n",
       " 2.283097062138199,\n",
       " 2.2782191415780613,\n",
       " 2.276993076405161,\n",
       " 2.269866791206342,\n",
       " 2.281609487158449,\n",
       " 2.270377095174182,\n",
       " 2.2707727919068468,\n",
       " 2.271844061495212,\n",
       " 2.271965681224016,\n",
       " 2.263619176988297,\n",
       " 2.270413184289831,\n",
       " 2.27423608521685,\n",
       " 2.266382214020636,\n",
       " 2.2626946820607814,\n",
       " 2.2678014665923594,\n",
       " 2.2351822217559185,\n",
       " 2.2703692058081466,\n",
       " 2.2761037161573334,\n",
       " 2.2568043232723056,\n",
       " 2.2747989135340814,\n",
       " 2.265677816491581,\n",
       " 2.2460004951388455,\n",
       " 2.273380082531615,\n",
       " 2.263753503552709,\n",
       " 2.2640331102225155,\n",
       " 2.262156340935488,\n",
       " 2.263662126738006,\n",
       " 2.2635210116613638,\n",
       " 2.2514017084479985,\n",
       " 2.2588915386868043,\n",
       " 2.256950722133798,\n",
       " 2.23437258912641,\n",
       " 2.2569745531819017,\n",
       " 2.250278875466316,\n",
       " 2.254129344332717,\n",
       " 2.263023058277555,\n",
       " 2.245842317553996,\n",
       " 2.2377805279853917,\n",
       " 2.2402818272148135,\n",
       " 2.2563154512491024,\n",
       " 2.2548320187925714,\n",
       " 2.2449435433821394,\n",
       " 2.251852133622246,\n",
       " 2.2435864814153037,\n",
       " 2.2536748490132923,\n",
       " 2.24134034538821,\n",
       " 2.244942081016091,\n",
       " 2.247815475880824,\n",
       " 2.2216090408010687,\n",
       " 2.2282796873759247,\n",
       " 2.229836414293066,\n",
       " 2.2163030100448795,\n",
       " 2.240770996750431,\n",
       " 2.2316022244810245,\n",
       " 2.22890801512612,\n",
       " 2.2233370208270666,\n",
       " 2.2218815333250044,\n",
       " 2.1964284098371123,\n",
       " 2.2128178490778914,\n",
       " 2.214570638031846,\n",
       " 2.2359863268681583,\n",
       " 2.2296767534099695,\n",
       " 2.2183843792190263,\n",
       " 2.20668364718252,\n",
       " 2.213363779717434,\n",
       " 2.193725065776255,\n",
       " 2.1943123163125575,\n",
       " 2.198269669027847,\n",
       " 2.1839731151407538,\n",
       " 2.2141022267177504,\n",
       " 2.234799685735452,\n",
       " 2.1891886160062475,\n",
       " 2.188012007024655,\n",
       " 2.186299090981607,\n",
       " 2.185705112780553,\n",
       " 2.1966868406880202,\n",
       " 2.1895998022424275,\n",
       " 2.1874546961003833,\n",
       " 2.15718632436557,\n",
       " 2.1687213542090764,\n",
       " 2.198645637501583,\n",
       " 2.1583689027854938,\n",
       " 2.173462370988431,\n",
       " 2.197398397042729,\n",
       " 2.1847382722046227,\n",
       " 2.181746251020674,\n",
       " 2.160203509768286,\n",
       " 2.175409001235146,\n",
       " 2.1666551817390958,\n",
       " 2.165898252971068,\n",
       " 2.1638092445370605,\n",
       " 2.155996308740333,\n",
       " 2.163301803471425,\n",
       " 2.113551516145471,\n",
       " 2.0986168306870083,\n",
       " 2.1476818586211777,\n",
       " 2.12246386336437,\n",
       " 2.1255222302377614,\n",
       " 2.120917040695961,\n",
       " 2.166927689308983,\n",
       " 2.1285616456565366,\n",
       " 2.10183041477856,\n",
       " 2.153164303835012,\n",
       " 2.1147322386308325,\n",
       " 2.088618524974839,\n",
       " 2.0727378651313204,\n",
       " 2.132445058604032,\n",
       " 2.095954108037399,\n",
       " 2.068904731270603,\n",
       " 2.0708513768797503,\n",
       " 2.089124931396091,\n",
       " 2.033378395611735,\n",
       " 2.0824798216336653,\n",
       " 2.0205325407597505,\n",
       " 2.0744384873817583,\n",
       " 2.0170565082839413,\n",
       " 2.065865804999197,\n",
       " 2.0604129524739925,\n",
       " 2.005345313332454,\n",
       " 2.0029601812481594,\n",
       " 2.0630106046025314,\n",
       " 2.0553341959594125,\n",
       " 2.0441806032298366,\n",
       " 2.0276088247544912,\n",
       " 1.9836016419989388,\n",
       " 2.0443853743888143,\n",
       " 2.0077769713408777,\n",
       " 1.977734873516527,\n",
       " 2.0473073781941262,\n",
       " 2.0357449123649163,\n",
       " 2.0153135942462854,\n",
       " 2.0048891790316787,\n",
       " 1.9890273332523296,\n",
       " 1.9942787879879165,\n",
       " 1.9460005514127316,\n",
       " 2.021465185361647,\n",
       " 1.9258209619923161,\n",
       " 2.0045171447171883,\n",
       " 2.0215470623210257,\n",
       " 2.003864747924591,\n",
       " 1.9782955226299743,\n",
       " 1.9907621572525809,\n",
       " 1.8971779711690906,\n",
       " 1.9264138715628314,\n",
       " 1.95513089633228,\n",
       " 1.919892573390647,\n",
       " 1.892785301358926,\n",
       " 1.9414628994766456,\n",
       " 1.9324732735388,\n",
       " 1.8703795281952011,\n",
       " 1.8423264149894374,\n",
       " 1.8981126289930565,\n",
       " 1.9084569848057646,\n",
       " 1.9262960095147819,\n",
       " 1.9094600169261902,\n",
       " 1.925066747090868,\n",
       " 1.8818029218617631,\n",
       " 1.8735395266877548,\n",
       " 1.8864300978847033,\n",
       " 1.8428986271358907,\n",
       " 1.8536091970147117,\n",
       " 1.8601003818995887,\n",
       " 1.840995891969678,\n",
       " 1.8283076457144112,\n",
       " 1.7631422492879354,\n",
       " 1.8237353344147647,\n",
       " 1.7621021773261771,\n",
       " 1.8492604033588915,\n",
       " 1.8450334392803085,\n",
       " 1.807608497574757,\n",
       " 1.8046158252840896,\n",
       " 1.7647867050582977,\n",
       " 1.7787885401186871,\n",
       " 1.8033406996476617,\n",
       " 1.7938622207687434,\n",
       " 1.8517488973888094,\n",
       " 1.7585849104650666,\n",
       " 1.7270324385534792,\n",
       " 1.770010669400332,\n",
       " 1.8090605800904986,\n",
       " 1.7850704523040783,\n",
       " 1.7438900004424318,\n",
       " 1.6694121315498855,\n",
       " 1.7810806234030756,\n",
       " 1.736784573897633,\n",
       " 1.6797116655261717,\n",
       " 1.6640697594580973,\n",
       " 1.6968671766043217,\n",
       " 1.6795138912163627,\n",
       " 1.740781850339896,\n",
       " 1.7422601708365262,\n",
       " 1.6877002785685875,\n",
       " 1.7658699719714712,\n",
       " 1.7225972889201062,\n",
       " 1.6323345056844858,\n",
       " 1.6507280385248646,\n",
       " 1.5864074343831052,\n",
       " 1.6511515871175968,\n",
       " 1.648171530099315,\n",
       " 1.635169773758061,\n",
       " 1.5160968364320522,\n",
       " 1.6215718313963063,\n",
       " 1.5603203636408653,\n",
       " 1.6759464740957557,\n",
       " 1.684153964520879,\n",
       " 1.739495508426944,\n",
       " 1.5787834738308084,\n",
       " 1.6321288996077528,\n",
       " 1.6434041627430105,\n",
       " 1.6290059593843267,\n",
       " 1.6291698245497164,\n",
       " 1.565911365726279,\n",
       " 1.493721117255159,\n",
       " 1.5334106326139483,\n",
       " 1.59967068809486,\n",
       " 1.529623744524641,\n",
       " 1.662369147251789,\n",
       " 1.5598160761773547,\n",
       " 1.569776503570903,\n",
       " 1.4374878984838737,\n",
       " 1.6370952295203132,\n",
       " 1.4969701718934019,\n",
       " 1.577521166219378,\n",
       " 1.5149914248380947,\n",
       " 1.5286664672014616,\n",
       " 1.5271072938435402,\n",
       " 1.5192580151362418,\n",
       " 1.480160907383902,\n",
       " 1.5500201116952976,\n",
       " 1.4860308102679591,\n",
       " 1.5063732223928321,\n",
       " 1.3966346490615609,\n",
       " 1.4451356318969995,\n",
       " 1.4944875215390432,\n",
       " 1.4548840938057919,\n",
       " 1.4607733927492423,\n",
       " 1.476271144944525,\n",
       " 1.4187607170599261,\n",
       " 1.437446316280469,\n",
       " 1.4803857004134238,\n",
       " 1.450334333439458,\n",
       " 1.5312157001432862,\n",
       " 1.392798275548125,\n",
       " 1.433893050326573,\n",
       " 1.3968412835812853,\n",
       " 1.4715186809899512,\n",
       " 1.5064810115204492,\n",
       " 1.4420261963180898,\n",
       " 1.3852833230154784,\n",
       " 1.4448525275828865,\n",
       " 1.5173330665193105,\n",
       " 1.4036016470189039,\n",
       " 1.367898271562068,\n",
       " 1.4280745918845872,\n",
       " 1.4700338726157518,\n",
       " 1.4163849474330532,\n",
       " 1.4360132138915385,\n",
       " 1.4831976089866443,\n",
       " 1.307490172585231,\n",
       " 1.3608787448943025,\n",
       " 1.3084849134896603,\n",
       " 1.432652392084694,\n",
       " 1.300716042771571,\n",
       " 1.5013428164446716,\n",
       " 1.3054076546470457,\n",
       " 1.3835383129501928,\n",
       " 1.3350895610545006,\n",
       " 1.3743504465227208,\n",
       " 1.3912527563377584,\n",
       " 1.3418830871161935,\n",
       " 1.3507204805454407,\n",
       " 1.2970398711661641,\n",
       " 1.2857823509293456,\n",
       " 1.3285322884092483,\n",
       " 1.292086459989134,\n",
       " 1.2632234333094972,\n",
       " 1.3961825635407594,\n",
       " 1.2867455358202102,\n",
       " 1.3431365403499742,\n",
       " 1.336234192123093,\n",
       " 1.2666346220290294,\n",
       " 1.3742307986093618,\n",
       " 1.2117008676488832,\n",
       " 1.4069294052524715,\n",
       " 1.239290762805458,\n",
       " 1.2605570439907685,\n",
       " 1.209561582134544,\n",
       " 1.2947221963996156,\n",
       " 1.2470380675512132,\n",
       " 1.1880934303307609,\n",
       " 1.264359725699155,\n",
       " 1.242291983203061,\n",
       " 1.195171785966448,\n",
       " 1.299835467625647,\n",
       " 1.2730909588570092,\n",
       " 1.2286132026218641,\n",
       " 1.2175537288957154,\n",
       " 1.272506013237825,\n",
       " 1.284029586742201,\n",
       " 1.3113299343633456,\n",
       " 1.2215698070871068,\n",
       " 1.2903514793552557,\n",
       " 1.2607232724958517,\n",
       " 1.2636299486880544,\n",
       " 1.1812330982277048,\n",
       " 1.2161220868555158,\n",
       " 1.2547568558006903,\n",
       " 1.1633328641891933,\n",
       " 1.2179093375443957,\n",
       " 1.267449960126272,\n",
       " 1.2383398812224435,\n",
       " 1.1578168216679914,\n",
       " 1.2098164181203064,\n",
       " 1.2870593119306815,\n",
       " 1.175221097572078,\n",
       " 1.1492050503369815,\n",
       " 1.1173776853485606,\n",
       " 1.0742108744656145,\n",
       " 1.2132432157769122,\n",
       " 1.2216854249551548,\n",
       " 1.2079652861033108,\n",
       " 1.221509147389255,\n",
       " 1.132045295045785,\n",
       " 1.121977887744839,\n",
       " 1.134249290649139,\n",
       " 1.1790399409640675,\n",
       " 1.1833425478571724,\n",
       " 1.1522877396563305,\n",
       " 1.1378679022685254,\n",
       " 1.217878600577047,\n",
       " 1.2300451271230692,\n",
       " 1.0811163946431566,\n",
       " 1.105218915826319,\n",
       " 1.116402250203824,\n",
       " 1.202706164251944,\n",
       " 1.101286971880823,\n",
       " 1.0792466501748201,\n",
       " 1.1089563126304207,\n",
       " 1.1487838290628027,\n",
       " 1.2467804123030695,\n",
       " 1.1274535628009352,\n",
       " 1.1063153129005214,\n",
       " 1.0850397877235467,\n",
       " 1.1157899755436258,\n",
       " 1.0705514970042012,\n",
       " 1.0729108739132889,\n",
       " 1.195612677222041,\n",
       " 1.0738648436491665,\n",
       " 1.1655166867632483,\n",
       " 1.0586719638920414,\n",
       " 1.0689393664652054,\n",
       " 1.048159629440879,\n",
       " 1.0579247034586292,\n",
       " 1.0520566720886624,\n",
       " 1.1466225571135877,\n",
       " 1.1958318355590374,\n",
       " 1.0908317316251483,\n",
       " 1.0269099946682407,\n",
       " 1.0581862774745239,\n",
       " 0.9816824369408356,\n",
       " 1.0405941659659221,\n",
       " 1.0446612353370077,\n",
       " 1.0943681448464864,\n",
       " 1.0170292434685912,\n",
       " 1.0232652635833797,\n",
       " 1.0746841227856005,\n",
       " 1.1218371174716857,\n",
       " 1.0474550268316887,\n",
       " 1.072347552397473,\n",
       " 1.0638111568271038,\n",
       " 1.0552084855386643,\n",
       " 0.9892714509165256,\n",
       " 0.9487435095596843,\n",
       " 0.9703478897525463,\n",
       " 1.0428259814155636,\n",
       " 0.9315739618287157,\n",
       " 1.1270452931345512,\n",
       " 1.0561645979911356,\n",
       " 1.0453931745433196,\n",
       " 1.0671033353883344,\n",
       " 0.9803120533654931,\n",
       " 1.0226187355588419,\n",
       " 0.8550895137733725,\n",
       " 0.9722558108507292,\n",
       " 0.9858539657503396,\n",
       " 1.0622282819463118,\n",
       " 0.942597089053135,\n",
       " 1.002625387785735,\n",
       " 1.0310795605650027,\n",
       " 0.9709834404838843,\n",
       " 1.000596918673081,\n",
       " 0.9212029714997917,\n",
       " 1.0656344224501997,\n",
       " 1.0001830301101051,\n",
       " 1.0840292501849296,\n",
       " 0.9567182270207194,\n",
       " 1.0347053578567023,\n",
       " 0.9270983738130235,\n",
       " 1.0226438494565495,\n",
       " 1.0771395436535394,\n",
       " 1.0172822649353097,\n",
       " 0.9429427642094906,\n",
       " 0.8888402136543542,\n",
       " 0.8744769024795198,\n",
       " 1.0131483399267873,\n",
       " 0.9686699803156513,\n",
       " 0.8799931292304793,\n",
       " 1.0312428538021086,\n",
       " 1.0618708011619211,\n",
       " 0.8936806853353161,\n",
       " 0.9537493915771136,\n",
       " 0.9037213482548749,\n",
       " 0.9242454711081166,\n",
       " 0.9543300068706092,\n",
       " 0.8920705586300399,\n",
       " 0.9682481194222522,\n",
       " 0.9120145341729132,\n",
       " 0.8688992978337352,\n",
       " 0.9494747385813623,\n",
       " 0.8840052909709594,\n",
       " 0.9150736050076577,\n",
       " 0.9088584284914031,\n",
       " 1.0464504060964868,\n",
       " 0.8814006701364775,\n",
       " 0.8938031967184145,\n",
       " 0.9463653482931998,\n",
       " 0.9164371631958527,\n",
       " 0.916040711746255,\n",
       " 0.940350038242312,\n",
       " 0.9136380561237537,\n",
       " 0.9901335750970742,\n",
       " 0.8769405764468655,\n",
       " 0.8659424680863875,\n",
       " 0.9092542750351201,\n",
       " 0.8836721073018474,\n",
       " 0.9106324562066278,\n",
       " 0.9587451583995261,\n",
       " 0.9465355296204598,\n",
       " 0.8880011077155336,\n",
       " 0.9186435566915735,\n",
       " 0.8283786268072428,\n",
       " 0.9489962995671317,\n",
       " 0.9187536136797001,\n",
       " 0.8252599152260346,\n",
       " 0.7922100326443138,\n",
       " 1.1064968632302419,\n",
       " 0.9277204129865453,\n",
       " 0.8801159186866729,\n",
       " 0.922318705007029,\n",
       " 0.8740577677842865,\n",
       " 0.9684215822888079,\n",
       " 0.886971785091163,\n",
       " 0.9705124916214402,\n",
       " 0.8664492383467552,\n",
       " 0.8697622519524008,\n",
       " 0.9532512683899362,\n",
       " 0.9690921987544954,\n",
       " 0.9696701263061658,\n",
       " 0.9115805196194634,\n",
       " 0.9259273954110754,\n",
       " 0.8066194448983187,\n",
       " 0.8318092247914928,\n",
       " 0.8823889132752873,\n",
       " 0.7404378046865147,\n",
       " 0.8815129692354067,\n",
       " 0.850296244724218,\n",
       " 0.9390293931430227,\n",
       " 0.8739537867187938,\n",
       " 0.8701724011529403,\n",
       " 0.7687286727914361,\n",
       " 0.8412906691850128,\n",
       " 0.8289418496571931,\n",
       " 0.8758509829301212,\n",
       " 0.8848863381868829,\n",
       " 0.8011281555236783,\n",
       " 0.7996132617492995,\n",
       " 0.856812369257732,\n",
       " 0.8481072322481564,\n",
       " 0.7527736887340535,\n",
       " 0.8744575941109051,\n",
       " 0.8556975705773326,\n",
       " 0.9528936575432151,\n",
       " 0.8629814999582109,\n",
       " 0.8131142027949005,\n",
       " 0.8270141411543726,\n",
       " 0.8541089134337713,\n",
       " 0.7154262309474467,\n",
       " 0.9005490257836102,\n",
       " 0.696814210898918,\n",
       " 0.810163070897856,\n",
       " 0.8542224286716907,\n",
       " 0.8596596339339606,\n",
       " 0.838313062181391,\n",
       " 0.7775940969889245,\n",
       " 0.859463599928551,\n",
       " 0.9941712976786988,\n",
       " 0.8775809069451477,\n",
       " 0.7390929702900236,\n",
       " 0.7059439907274973,\n",
       " 0.8307764643656991,\n",
       " 0.8238027589420187,\n",
       " 0.7056053977856928,\n",
       " 0.8648055835692858,\n",
       " 0.9586647442463745,\n",
       " 0.8822218702334655,\n",
       " 0.8191206689462132,\n",
       " 0.7344664278953671,\n",
       " 0.755641514256732,\n",
       " 0.8062870383038842,\n",
       " 0.9097222945820133,\n",
       " 0.7647304291733991,\n",
       " 0.7404600415012449,\n",
       " 0.83421973571434,\n",
       " 0.7808934052135331,\n",
       " 0.811922431311926,\n",
       " 0.83150205356336,\n",
       " 0.8396370394409991,\n",
       " 0.7152550200892136,\n",
       " 0.7303115972318405,\n",
       " 0.7639966617880987,\n",
       " 0.8365425341347692,\n",
       " 0.7388768300310964,\n",
       " 0.857213974745541,\n",
       " 0.8492956976020836,\n",
       " 0.7788174797163896,\n",
       " 0.7433231546244934,\n",
       " 0.8195845972211374,\n",
       " 0.721956316441787,\n",
       " 0.7985839139319171,\n",
       " 0.7055631740396292,\n",
       " 0.755746851984397,\n",
       " 0.8946014311105533,\n",
       " 0.8062698254256323,\n",
       " 0.8065237920479668,\n",
       " 0.7851556915300328,\n",
       " 0.7593172711562876,\n",
       " 0.749697552326124,\n",
       " 0.729718112923895,\n",
       " 0.7534296416798751,\n",
       " 0.7545049470504437,\n",
       " 0.7237895398132072,\n",
       " 0.7611580456503464,\n",
       " 0.6868293144402001,\n",
       " 0.8096736956757951,\n",
       " 0.7051185065088156,\n",
       " 0.7573708945438739,\n",
       " 0.8060109548591123,\n",
       " 0.6747343094037674,\n",
       " 0.7894614735959494,\n",
       " 0.7674481802114775,\n",
       " 0.8050098912444994,\n",
       " 0.803715296398089,\n",
       " 0.6471942630503231,\n",
       " 0.7579669233603888,\n",
       " 0.7870989501932972,\n",
       " 0.7037672710148262,\n",
       " 0.7631075664053418,\n",
       " 0.7049772507528406,\n",
       " 0.7090187207032891,\n",
       " 0.797296580765714,\n",
       " 0.8177325470024911,\n",
       " 0.6354385383016402,\n",
       " 0.7285581482494279,\n",
       " 0.7059077455408604,\n",
       " 0.6589283129792814,\n",
       " 0.6455110328124916,\n",
       " 0.6977122312498917,\n",
       " 0.7626689747408082,\n",
       " 0.6837549468761217,\n",
       " 0.6990599471097205,\n",
       " 0.7336805493755932,\n",
       " 0.8617136561644497,\n",
       " 0.6559176392794006,\n",
       " 0.7537848494317514,\n",
       " 0.6711138395853742,\n",
       " 0.6350223920975853,\n",
       " 0.6807928114802247,\n",
       " 0.7489070097947282,\n",
       " 0.6704479301584559,\n",
       " 0.8121584373528142,\n",
       " 0.7095564994452311,\n",
       " 0.8745104405734259,\n",
       " 0.6777614190932639,\n",
       " 0.7137918741406267,\n",
       " 0.7407851188008283,\n",
       " 0.6984493978621521,\n",
       " 0.7184513726924772,\n",
       " 0.6195774555756444,\n",
       " 0.614322745773027,\n",
       " 0.6990606754722378,\n",
       " 0.7284379874286305,\n",
       " 0.6573242867104426,\n",
       " 0.8307251204640568,\n",
       " 0.8270183448897355,\n",
       " 0.7003707589340575,\n",
       " 0.6672112299049008,\n",
       " 0.7033445272805277,\n",
       " 0.6935007498245167,\n",
       " 0.6804340244138893,\n",
       " 0.7241518409139823,\n",
       " 0.6573962266874857,\n",
       " 0.6403046365009607,\n",
       " 0.6323468063680238,\n",
       " 0.6884701748179713,\n",
       " 0.7676995236525839,\n",
       " 0.5445048208675285,\n",
       " 0.6670026904162439,\n",
       " 0.6869364483403989,\n",
       " 0.6723190957744785,\n",
       " 0.6745622838633619,\n",
       " 0.6735090715717419,\n",
       " 0.8130987422014454,\n",
       " 0.6651277127801943,\n",
       " 0.6954539502474408,\n",
       " 0.7117279939817316,\n",
       " 0.6272193861786706,\n",
       " 0.6969692691812979,\n",
       " 0.6420129518106418,\n",
       " 0.6437985603811656,\n",
       " 0.7434132936305456,\n",
       " 0.7005023925638987,\n",
       " 0.7096606843472071,\n",
       " 0.7035474435946019,\n",
       " 0.760276537338115,\n",
       " 0.8592818150635547,\n",
       " 0.6905305146401949,\n",
       " 0.7657163411728337,\n",
       " 0.5460716709139858,\n",
       " 0.6731750654145514,\n",
       " 0.6344134011019666,\n",
       " 0.6878600800024699,\n",
       " 0.6706329601193411,\n",
       " 0.6519600527836253,\n",
       " 0.6360951207713815,\n",
       " 0.6959834550727751,\n",
       " 0.8977907413401358,\n",
       " 0.6360654938071196,\n",
       " 0.7087391269712886,\n",
       " 0.6774237659916416,\n",
       " 0.6195007690850521,\n",
       " 0.6624746500856213,\n",
       " 0.7593013043231376,\n",
       " 0.575423855579406,\n",
       " 0.5967505972738172,\n",
       " 0.6586205684420936,\n",
       " 0.6741502137943766,\n",
       " 0.6130254583579776,\n",
       " 0.7009462547002272,\n",
       " 0.6550592857593952,\n",
       " 0.6179464848137776,\n",
       " 0.7838919459488437,\n",
       " 0.7817433808088243,\n",
       " 0.6222289525121006,\n",
       " 0.5897687862268511,\n",
       " 0.7600509548810508,\n",
       " 0.6645833942843659,\n",
       " 0.7175279401966033,\n",
       " 0.6823029241930552,\n",
       " 0.5579166793292342,\n",
       " 0.7659832764847512,\n",
       " 0.6056826381959312,\n",
       " 0.6078422972905437,\n",
       " 0.7091469115988187,\n",
       " 0.5895961519196422,\n",
       " 0.5504285645713215,\n",
       " 0.6892287947997414,\n",
       " 0.6643159722094105,\n",
       " 0.6109728620339792,\n",
       " 0.7076509149998594,\n",
       " 0.66295763557439,\n",
       " 0.6081002630076938,\n",
       " 0.6236440999850376,\n",
       " 0.5722183077570074,\n",
       " 0.6045622458772835,\n",
       " 0.6425931694084038,\n",
       " 0.6327887803377863,\n",
       " 0.5746845250999661,\n",
       " 0.5985770759215587,\n",
       " 0.7563259513911145,\n",
       " 0.5767876901094643,\n",
       " 0.7065709864193933,\n",
       " 0.6254295371714017,\n",
       " 0.5788173402645211,\n",
       " 0.5909423745682499,\n",
       " 0.5478378207750552,\n",
       " 0.6413566462047272,\n",
       " 0.6276971272502958,\n",
       " 0.7063610626761755,\n",
       " 0.7968185921181121,\n",
       " 0.6590598226120207,\n",
       " 0.56701980014398,\n",
       " 0.5312249289685271,\n",
       " 0.5819360865011692,\n",
       " 0.6244000777097516,\n",
       " 0.6913903202680192,\n",
       " 0.5454266243081124,\n",
       " 0.6593672494629109,\n",
       " 0.6117050149247613,\n",
       " 0.6453186536004849,\n",
       " 0.6273925306732293,\n",
       " 0.59289150963718,\n",
       " 0.6377063643307185,\n",
       " 0.6211139477647183,\n",
       " 0.6548429930377998,\n",
       " 0.644824171762845,\n",
       " 0.7546914474497849,\n",
       " 0.6384331445615511,\n",
       " 0.7426178583902684,\n",
       " 0.6050124406276295,\n",
       " 0.5083119100492651,\n",
       " 0.5004881004462483,\n",
       " 0.712662356049739,\n",
       " 0.5635557629016055,\n",
       " 0.598582238592089,\n",
       " 0.6849088647391213,\n",
       " 0.6733576784428393,\n",
       " 0.5299168227772836,\n",
       " 0.5712458124945111,\n",
       " 0.6197079283162918,\n",
       " 0.6818968420297477,\n",
       " 0.5632920372489236,\n",
       " 0.6507909792850458,\n",
       " 0.4666526768486754,\n",
       " 0.6613394147982562,\n",
       " 0.650712527183331,\n",
       " 0.7779264711652154,\n",
       " 0.5111758395965983,\n",
       " 0.5923271185517901,\n",
       " 0.5358021659411447,\n",
       " 0.6261630703017589,\n",
       " 0.5164118723105073,\n",
       " 0.6536649477706556,\n",
       " 0.6113251982189178,\n",
       " 0.524476104690807,\n",
       " 0.5020703192500183,\n",
       " 0.5770317816131175,\n",
       " 0.59062651357202,\n",
       " 0.6193381432084099,\n",
       " 0.5266382372078516,\n",
       " 0.6140719201125571,\n",
       " 0.4926238455686842,\n",
       " 0.6151871420947107,\n",
       " 0.6284099191126882,\n",
       " 0.5522495191856431,\n",
       " 0.7791390676183759,\n",
       " 0.5262081363824009,\n",
       " 0.6324734713574484,\n",
       " 0.5577140301622244,\n",
       " 0.5722426822997387,\n",
       " 0.5115306229527248,\n",
       " 0.46940151905390465,\n",
       " 0.6249812100096406,\n",
       " 0.5816396803984302,\n",
       " 0.5911461061936669,\n",
       " 0.5733924033368768,\n",
       " 0.5447643191055997,\n",
       " 0.6523235638742156,\n",
       " 0.6413911137710291,\n",
       " 0.5509003884561232,\n",
       " 0.48920249005596017,\n",
       " 0.4928120653654042,\n",
       " 0.574456919482199,\n",
       " 0.6801571553541766,\n",
       " 0.6549114078280123,\n",
       " 0.596102481391408,\n",
       " 0.594345151672618,\n",
       " 0.522903449809451,\n",
       " 0.6294389902366018,\n",
       " 0.5091039129368412,\n",
       " 0.5522241192901334,\n",
       " 0.5413951363112779,\n",
       " 0.6167808265838162,\n",
       " 0.5595883392379276,\n",
       " 0.5964504138433967,\n",
       " 0.6265956867753597,\n",
       " 0.6203847389392754,\n",
       " 0.641494066783109,\n",
       " 0.692889808711484,\n",
       " 0.6980788814773429,\n",
       " 0.5542301270325516,\n",
       " 0.5273748977170073,\n",
       " 0.5554376501997962,\n",
       " 0.48254791295079735,\n",
       " 0.48047579721446654,\n",
       " 0.6156214124293267,\n",
       " 0.5592597438990156,\n",
       " 0.5958891777902544,\n",
       " 0.6813713790356367,\n",
       " 0.6397328764056359,\n",
       " 0.508333677528487,\n",
       " 0.6082129746660414,\n",
       " 0.5387150122858921,\n",
       " 0.5749341357474381,\n",
       " 0.5768113516569512,\n",
       " 0.5144371102808356,\n",
       " 0.5830332139882218,\n",
       " 0.7558799440923972,\n",
       " 0.5880531742573638,\n",
       " 0.692625715722994,\n",
       " 0.5313401893436925,\n",
       " 0.5514952688867377,\n",
       " 0.5846273594670381,\n",
       " 0.5037783764334438,\n",
       " 0.6319241045889252,\n",
       " 0.5602586036787323,\n",
       " 0.6985736093574824,\n",
       " 0.619395519919043,\n",
       " 0.6718847136192686,\n",
       " 0.46582071416290305,\n",
       " 0.6048369542054595,\n",
       " 0.5528848986419466,\n",
       " 0.5789512953205055,\n",
       " 0.5704869225437423,\n",
       " 0.5255413774637939,\n",
       " 0.5458375511652713,\n",
       " 0.4239140937060618,\n",
       " 0.5276990442193358,\n",
       " 0.7702414958260978,\n",
       " 0.4561526886884233,\n",
       " 0.4968993485544136,\n",
       " 0.4787460523971706,\n",
       " 0.5849300896172906,\n",
       " 0.6411162194961894,\n",
       " 0.5194512518175669,\n",
       " 0.5823498690156457,\n",
       " 0.5512931610697368,\n",
       " 0.513712829771881,\n",
       " 0.4730242018736044,\n",
       " 0.6651282409842508,\n",
       " 0.5216256002433765,\n",
       " 0.5455176026712693,\n",
       " 0.5698235728539983,\n",
       " 0.4865154138402253,\n",
       " 0.5660261344515248,\n",
       " 0.5043658302079811,\n",
       " 0.5364931927786639,\n",
       " 0.5262194016604327,\n",
       " 0.5332237426943566,\n",
       " 0.4843438905689271,\n",
       " 0.6903019994088899,\n",
       " 0.48138968413937044,\n",
       " 0.6471087911004807,\n",
       " 0.568347320892774,\n",
       " 0.4779168810041481,\n",
       " 0.4858114570325,\n",
       " 0.5309945566341562,\n",
       " 0.5798376968396296,\n",
       " 0.5624092129195575,\n",
       " 0.5052572311320955,\n",
       " 0.4680855443203565,\n",
       " 0.5326402542252635,\n",
       " 0.5288864023166538,\n",
       " 0.5274811892895986,\n",
       " 0.5325268279023315,\n",
       " 0.55457917350814,\n",
       " 0.4123344640578263,\n",
       " 0.5827296962562163,\n",
       " 0.5358721786190176,\n",
       " 0.5159411480230165,\n",
       " 0.5858436739301554,\n",
       " 0.5029958668523915,\n",
       " 0.517149182629441,\n",
       " 0.5948641086109032,\n",
       " 0.5147750220887898,\n",
       " 0.6939250293645874,\n",
       " 0.5653157540678406,\n",
       " 0.6746694474788332,\n",
       " 0.4929120752427801,\n",
       " 0.5602348719621816,\n",
       " 0.5788959263862793,\n",
       " 0.44618288244986365,\n",
       " 0.5537020365690917,\n",
       " 0.5558750872103383,\n",
       " 0.4838624448101555,\n",
       " 0.49176414737948937,\n",
       " 0.5438608691927875,\n",
       " 0.5355533293712709,\n",
       " 0.43648795411763586,\n",
       " 0.5030185522733334,\n",
       " 0.6263604426351407,\n",
       " 0.5596848564373272,\n",
       " 0.5483495093245859,\n",
       " 0.5795327779552978,\n",
       " 0.5202883387085082,\n",
       " 0.5352547724071567,\n",
       " 0.5102264656070247,\n",
       " 0.5242423465880396,\n",
       " 0.6334250736124198,\n",
       " 0.5295609887159977,\n",
       " 0.5742425238726876,\n",
       " 0.4631530729841865,\n",
       " 0.40608575926661317]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "# hyperparameter\n",
    "iters_num = 1000 # 반복 횟수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100 # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 1000번 돌린다. \n",
    "for i in range(iters_num):\n",
    "    # 600000개의 훈련데이터에서 100개를 무작위로 선택한다. \n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask] # 무작위로 선택된 100개의 입력 데이터\n",
    "    t_batch = t_train[batch_mask] # 그 결과값인 100개의 레이블 데이터\n",
    "    \n",
    "    # 미분에 의한 기울기 벡터를 구함: 속도가 매우 느리다.\n",
    "    # grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 오차 역전파에 의해 기울기 벡터를 구함\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        # 각각의 가중치를 초기 랜덤값에서 음의 기울기 방향으로 가중치를 갱신한다. (즉, 하강한다.)\n",
    "        # 이 때 학습률(learning rate)의 값으로 갱신량을 결정한다. 오버 피팅이 발생하지 않도록 적절한 값을 산정한다.\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # 학습 경과 \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "train_loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"equations_and_figures/fig%204-11.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.5.3 시험 데이터로 평가하기\n",
    "### NOTE_ \n",
    "- 에폭(epoch) 은 하나의 단위이다.\n",
    "- 훈련 데이터가 총 10,000개이고 100개를 하나의 미니배치로 학습할경우\n",
    "- 1 에폭은 100회가 된다. 그러니깐, 모든 훈련 데이터를 '소진' 하는게 1 에폭이다. \n",
    "- 다음페이지는 실제 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc: 0.099, test_acc: 0.098\n",
      "train_acc: 0.783, test_acc: 0.788\n",
      "train_acc: 0.878, test_acc: 0.882\n",
      "train_acc: 0.899, test_acc: 0.900\n",
      "train_acc: 0.908, test_acc: 0.910\n",
      "train_acc: 0.913, test_acc: 0.916\n",
      "train_acc: 0.919, test_acc: 0.922\n",
      "train_acc: 0.924, test_acc: 0.925\n",
      "train_acc: 0.928, test_acc: 0.929\n",
      "train_acc: 0.931, test_acc: 0.933\n",
      "train_acc: 0.934, test_acc: 0.934\n",
      "train_acc: 0.937, test_acc: 0.936\n",
      "train_acc: 0.939, test_acc: 0.938\n",
      "train_acc: 0.941, test_acc: 0.941\n",
      "train_acc: 0.943, test_acc: 0.941\n",
      "train_acc: 0.946, test_acc: 0.943\n",
      "train_acc: 0.947, test_acc: 0.945\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size/batch_size, 1)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 60000개의 훈련 데이터에서 임의로 100개를 선택한다.\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # 학습 경과 \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train_acc: {:0.3f}, test_acc: {:0.3f}\".format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"equations_and_figures/fig%204-12.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 정리\n",
    "- 기계학습 사용 데이터는 훈련 데이터와 시험 데이터로 나눈다.\n",
    "- 훈련 데이터로 학습한 모델의 범용 능력을 시험 데이터로 평가한다.\n",
    "- 신경망 학습은 손실 함수를 지표로, 손실 함수 값이 작아지는 방향으로 매개변수를 갱신한다.\n",
    "- 가중치 매개변수를 갱신할 때는 기울기를 이용하고, 기울어진 방향으로 가중치 값을 갱신하는 작업을 반복한다.\n",
    "- 미분을 해서 가중치 매개변수의 기울기를 계산할 수 있다.\n",
    "- 다음 장에는 오차역전파법으로 고속으로 기울기를 계산하는 법을 배운다."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
